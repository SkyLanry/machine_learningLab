{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、Mindspore框架\n",
    "\n",
    "本节通过使用Mindspore框架训练神经网络实现图像分类，初步了解深度学习的主要过程。图像分类任务选择的是经典的手写字符体识别，主要步骤如下：\n",
    "\n",
    "1. 加载和预处理MNIST数据集；\n",
    "2. 定义模型，包括损失函数和激活函数，本节使用LeNet网络；\n",
    "3. 加载数据使用优化算法进行模型权重更新；\n",
    "4. 重复前面的步骤指定的轮数(epochs)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Mindspore安装\n",
    "\n",
    "Mindspore的安装方式请参考官方手册[MindSpore安装](https://www.mindspore.cn/install)，根据自己所用的环境选择合适的安装方法，尽量选择mindspore 2.2.14版本conda安装命令。\n",
    "Mindpsore 安装后运行以下命令测试是否安装成功：\n",
    "\n",
    "```python -c \"import mindspore;mindspore.set_context(device_target='CPU');mindspore.run_check()\"```\n",
    "\n",
    "如果输出以下内容则成功安装。\n",
    "```\n",
    "MindSpore version: 版本号\n",
    "The result of multiplication calculation is correct, MindSpore has been installed on platform [CPU] successfully!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 MNIST数据集\n",
    "\n",
    "`MNIST`手写字符体数据集是机器学习领域比较经典的，也是入门常用的数据集。于1998年组合NIST的SD-3和SD-1数据集而成，有60000张训练图片，10000张测试图片。包含0-9共10类数字，每张图片是单通道灰度图，大小都是28*28，字符体居中。我们要实现的就是训练神经网络能够识别这些数字，输入一张图片，输出识别后的数字即标签。\n",
    "\n",
    "对于计算机来说，灰度图像是一个由像素点组成的巨大的矩阵。每个数字都是在范围0-255之间的整型，其中0表示全黑，255表示全白。\n",
    "\n",
    "[MNIST dataset](http://yann.lecun.com/exdb/mnist/index.html) 是MNIST数据集的主页，介绍了MNIST数据集的由来以及2012年之前不同方法在MNIST上的准确率。完整的MNIST数据集下载下来包含以下4个文件，前2个文件是训练数据和训练数据对应的标签，后2个文件是测试集和测试集对应的标签。\n",
    "```\n",
    "train-images-idx3-ubyte.gz:  training set images (9912422 bytes)\n",
    "train-labels-idx1-ubyte.gz:  training set labels (28881 bytes)\n",
    "t10k-images-idx3-ubyte.gz:   test set images (1648877 bytes)\n",
    "t10k-labels-idx1-ubyte.gz:   test set labels (4542 bytes)\n",
    "```\n",
    "\n",
    "gz格式为gzip压缩文件，使用wget命令下载训练图片之后再用gunzip解压，可以得到解压缩后的IDX文件train-images-idx3-ubyte，无法直接用图片软件打开，因为单个文件打包了60000张训练图片，读取后按照idx的格式解析才能读出图片数据。除了从官网下载，github等平台也有MNIST的完整副本，很多框架将MNIST等经典数据集的下载、解压、加载、预处理做了很好的封装，本节从华为云存储上下载MNIST数据集的副本用于测试。\n",
    "\n",
    "MindSpore提供基于Pipeline的数据引擎，通过数据集（Dataset）和数据变换（Transforms）实现高效的数据预处理。在本教程中，下载MNIST之后，使用mindspore.dataset提供的数据变换MnistDataset进行预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data from open datasets\n",
    "# pip install download\n",
    "from download import download\n",
    "mindspore_mnist_dir = './src/dataset/'\n",
    "url = \"https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/\" \\\n",
    "      \"notebook/datasets/MNIST_Data.zip\"\n",
    "path = download(url, mindspore_mnist_dir, kind=\"zip\", replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:53:42.128603Z",
     "start_time": "2021-02-03T08:53:41.530089Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mindspore\n",
    "import matplotlib.pyplot as plt\n",
    "from mindspore.dataset import vision, transforms\n",
    "from mindspore.dataset import MnistDataset\n",
    "\n",
    "mnist_train_dir = mindspore_mnist_dir + 'MNIST_Data/train'\n",
    "mnist_test_dir = mindspore_mnist_dir + 'MNIST_Data/test'\n",
    "mnist_sample = MnistDataset(mnist_test_dir)\n",
    "dic_ds = mnist_sample.create_dict_iterator()\n",
    "item = next(dic_ds)            # 取出一个样本\n",
    "img = item[\"image\"].asnumpy()\n",
    "label = item[\"label\"].asnumpy()\n",
    "\n",
    "print(\"Tensor of image in item:\", img.shape) \n",
    "print(\"The label of item:\", label)\n",
    "img = np.squeeze(img)  # 去掉维度为1的轴,(28, 28, 1) --> (28, 28)\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "np.set_printoptions(linewidth=160)\n",
    "# 原始数据像素值为0-255\n",
    "print(img)\n",
    "\n",
    "plt.imshow(img,cmap=\"gray\")  # 默认是彩色输出\n",
    "plt.title(\"number:%s\"% item[\"label\"].asnumpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MindSpore提供基于Pipeline的数据引擎（Data Processing Pipeline），需指定map、batch、shuffle等操作。这里使用map对图像数据及标签进行变换处理，然后将处理好的数据集打包为大小为指定大小的的batch。创建数据集对象后，可使用create_tuple_iterator 或create_dict_iterator对数据集进行迭代访问。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得数据集对象\n",
    "train_dataset = MnistDataset(mnist_train_dir, num_parallel_workers=1)\n",
    "test_dataset = MnistDataset(mnist_test_dir, num_parallel_workers=1)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "def datapipe(dataset, batch_size): \n",
    "    image_transforms = [\n",
    "        vision.Rescale(1.0 / 255.0, 0),\n",
    "        vision.Normalize(mean=(0.1307,), std=(0.3081,)),\n",
    "        vision.HWC2CHW()\n",
    "    ]\n",
    "    label_transform = transforms.TypeCast(mindspore.int32)\n",
    "\n",
    "    dataset = dataset.map(image_transforms, 'image')\n",
    "    dataset = dataset.map(label_transform, 'label', num_parallel_workers=1)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "# Map vision transforms and batch dataset\n",
    "train_dataset = datapipe(train_dataset, batch_size)\n",
    "test_dataset = datapipe(test_dataset, batch_size)\n",
    "print(\"batch size is \", train_dataset.batch_size)\n",
    "print(\"Total batches contained in the train_dataset \", train_dataset.get_dataset_size())\n",
    "print(\"Total batches contained in the test_dataset\", test_dataset.get_dataset_size())\n",
    "\n",
    "\n",
    "print(\"create_tuple_iterator:\")\n",
    "for image, label in train_dataset.create_tuple_iterator():\n",
    "    print(f\"Shape of image [N, C, H, W]: {image.shape} {image.dtype}\")\n",
    "    print(f\"Shape of label: {label.shape} {label.dtype}\")\n",
    "    break\n",
    "\n",
    "print(\"create_dict_iterator:\")\n",
    "for data in test_dataset.create_dict_iterator():\n",
    "    print(f\"Shape of image [N, C, H, W]: {data['image'].shape} {data['image'].dtype}\")\n",
    "    print(f\"Shape of label: {data['label'].shape} {data['label'].dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面将加载后的图片和`label`进行对应并可视化，每调用一次next会获得batch_size=64张图片，显示前32张。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:53:43.245788Z",
     "start_time": "2021-02-03T08:53:42.582357Z"
    }
   },
   "outputs": [],
   "source": [
    "count = 1\n",
    "data = next(train_dataset.create_dict_iterator(output_numpy=True))\n",
    "images = data[\"image\"]\n",
    "labels = data[\"label\"]\n",
    "print('Images shape:', images.shape)\n",
    "print('Labels:', labels)\n",
    "for i in images[0:32]:\n",
    "    plt.subplot(4, 8, count) \n",
    "    plt.imshow(np.squeeze(i), cmap=\"gray\")\n",
    "    plt.title('num:%s'%labels[count-1])\n",
    "    plt.xticks([])\n",
    "    count += 1\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 定义网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们选择相对简单的[LeNet网络](http://yann.lecun.com/exdb/lenet/)。LeNet，最早发布的卷积神经网络之一，因其在计算机视觉任务中的高效性能而受到广泛关注。这个模型是由AT&T贝尔实验室的研究员Yann LeCun在1989年提出的（并以其命名），目的是识别图像中的手写数字，LeNet取得了与支持向量机（support vector machines）性能相媲美的成果。1998年LeCun训练出的LeNet-5模型在MNIST上能达到99.2%的准确率，并且有很好的鲁棒性。当时用传统机器学习算法如KNN、SVM也能达到99%的准确率。根据[MNIST database wikipedia](https://en.wikipedia.org/wiki/MNIST_database)的统计，最低的错误率做到了0.09%。\n",
    "\n",
    "下图是LeNet论文中的模型结构图：\n",
    "\n",
    "<img src=\"./notebook-imgs/LeNet-5.png\" width=\"800\">\n",
    "\n",
    "输入图像尺寸为 32×32，比 MNIST 数据集中的尺寸（28*28）要大，需要对原始图像做 resize 处理。这样做的目的是希望潜在的明显特征，如笔画断续、角点，能够出现在最高层特征监测卷积核的中心。池化层用的是平均池化，激活函数用sigmoid，最后一层是使用高斯激活即Euclidean Radial Basis Function units(RBF)单元的全连接层，RBF现已不再使用。输⼊是⼿写数字，输出为大小为10的向量，分别对应每种字符可能出现的概率，概率取值最大的index为识别出来的字符。对于分类任务，有多少个类别，输出层就有多少个节点。\n",
    "\n",
    "\n",
    "本实验，我们实现一个简化后的LeNet，模型结构和数据流如下图，输入MNIST图片大小为`28*28`，第一个卷积层设置了padding=2，使得效果等同于输入为32*32，激活函数用ReLU，输出去掉了最后一层的高斯激活。\n",
    "\n",
    "<img src=\"./notebook-imgs/lenet-simple.svg\" width=\"800\">\n",
    "\n",
    "LeNet网络不包括输入层的情况下，共有7层：2个卷积层、2个下采样层（池化层）、3个全连接层。每层的参数配置如下图所示：\n",
    "\n",
    "<img src=\"./notebook-imgs/lenet-vert.svg\" width=\"200\">\n",
    "\n",
    "全连接层以及卷积层采用正态分布函数Normal()进行参数初始化，MindSpore支持`TruncatedNormal`、`Normal`、`Uniform`等多种初始化方法。\n",
    "\n",
    "深度学习框架通常会提供不同层次的API用于模型构建，本实验使用`mindspore.nn`接口，定义神经网络需要继承`mindspore.nn.Cell`类，`Cell`是所有神经网络（`Conv2d`等）的基类。神经网络的各层在`__init__`方法中定义，然后根据网络结构，在`construct`方法中完成神经网络的前向构造。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-03T08:53:43.367791Z",
     "start_time": "2021-02-03T08:53:43.248322Z"
    }
   },
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "from mindspore.common.initializer import Normal\n",
    "\n",
    "\n",
    "class LeNet5(nn.Cell):\n",
    "    \"\"\"Lenet network structure.\"\"\"\n",
    "    # define the operator required\n",
    "    def __init__(self, num_class=10, num_channel=1):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channel, 6, 5, pad_mode='pad', padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, pad_mode='valid')\n",
    "        self.fc1 = nn.Dense(16 * 5 * 5, 120, weight_init=Normal(0.02))\n",
    "        self.fc2 = nn.Dense(120, 84, weight_init=Normal(0.02))\n",
    "        self.fc3 = nn.Dense(84, num_class, weight_init=Normal(0.02))\n",
    "        self.relu = nn.ReLU() \n",
    "        self.avg_pool2d = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        # self.dropout = nn.Dropout(0.5)   # dropout层\n",
    "\n",
    "    # use the preceding operators to construct networks\n",
    "    def construct(self, x):\n",
    "        x = self.avg_pool2d(self.relu(self.conv1(x)))\n",
    "        x = self.avg_pool2d(self.relu(self.conv2(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        #x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "model = LeNet5()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 模型训练\n",
    "\n",
    "定义好模型后，还需要选择合适的损失函数和优化器，这里我们选择交叉熵损失函数`nn.CrossEntropyLoss()`和随机梯度下降优化器`nn.SGD()`。\n",
    "\n",
    "在模型训练中，一个完整的训练过程（step）需要实现以下三步：\n",
    "\n",
    "1. 正向计算：模型预测结果（logits），并与正确标签（label）求预测损失（loss）。\n",
    "2. 反向传播：利用自动微分机制，自动求模型参数（parameters）对于loss的梯度（gradients）。\n",
    "3. 参数优化：将梯度更新到参数上。\n",
    "\n",
    "MindSpore使用函数式自动微分机制，因此针对上述步骤需要实现：\n",
    "1. 定义正向计算函数。\n",
    "2. 使用value_and_grad()通过函数变换获得梯度计算函数。\n",
    "3. 定义训练函数，使用set_train设置为训练模式，执行正向计算、反向传播和参数优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = nn.SGD(model.trainable_params(), 1e-2)\n",
    "\n",
    "# 1. Define forward function\n",
    "def forward_fn(data, label):\n",
    "    logits = model(data)\n",
    "    loss = loss_fn(logits, label)\n",
    "    return loss, logits\n",
    "\n",
    "# 2. Get gradient function\n",
    "grad_fn = mindspore.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "\n",
    "# 3. Define function of one-step training\n",
    "def train_step(data, label):\n",
    "    (loss, logits), grads = grad_fn(data, label)\n",
    "    optimizer(grads)\n",
    "    return loss, logits\n",
    "\n",
    "def train(model, dataset):\n",
    "    num_batches = dataset.get_dataset_size()\n",
    "    model.set_train()\n",
    "    for batch, (data, label) in enumerate(dataset.create_tuple_iterator()):\n",
    "        loss, logits = train_step(data, label)\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.asnumpy(), batch\n",
    "            correct = (logits.argmax(1) == label).asnumpy().sum()  # 只计算当前batch的准确率\n",
    "            acc = correct/batch_size\n",
    "            print(f\"loss: {loss:>7f}  [{current:>3d}/{num_batches:>3d}], accuray:{acc:>7f} \")\n",
    "\n",
    "# 训练过程中，除训练外，通常会定义测试函数，用来评估模型在测试集上的效果，用于判断是否过拟合\n",
    "def test(model, dataset, loss_fn):\n",
    "    num_batches = dataset.get_dataset_size()\n",
    "    model.set_train(False)\n",
    "    total, test_loss, correct = 0, 0, 0\n",
    "    for data, label in dataset.create_tuple_iterator():\n",
    "        pred = model(data)\n",
    "        total += len(data)\n",
    "        test_loss += loss_fn(pred, label).asnumpy()\n",
    "        correct += (pred.argmax(1) == label).asnumpy().sum()\n",
    "    test_loss /= num_batches\n",
    "    correct /= total\n",
    "    print(f\"Test: \\n Avg loss: {test_loss:>8f}, Accuracy: {(100*correct):>0.1f}% \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练过程需多次迭代数据集，数据集一次完整的遍历称为一轮（epoch）。训练过程，通常会打印每一轮的loss值和预测准确率（Accuracy）用来观测训练的效果和进展，可以看到loss在不断下降，Accuracy在不断提高。windows下该代码可能运行会非常慢，CPU利用率也特别低。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(model, train_dataset)\n",
    "    test(model, test_dataset, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 推理（预测）\n",
    "\n",
    "模型训练完成后，可以对图片进行识别测试，这里我们从test_dataset选择部分图片进行测试：首先将模型设置为非训练模式，再将数据送入模型进行前向计算，根据输出的向量得出预测的标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_train(False)\n",
    "for data, label in test_dataset:\n",
    "    pred = model(data)\n",
    "    predicted = pred.argmax(1)\n",
    "    print(f'Predicted: \"{predicted[:10]}\"')\n",
    "    print(f'Actual   : \"{label[:10]}\"')\n",
    "\n",
    "    count = 1\n",
    "    for img in data[:10].asnumpy():\n",
    "        plt.subplot(2, 5, count) \n",
    "        plt.imshow(np.squeeze(img), cmap=\"gray\")\n",
    "        plt.title('num:%s' % predicted[count-1])\n",
    "        plt.xticks([])\n",
    "        count += 1\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节需要完成的：\n",
    "1. 安装Mindspore\n",
    "2. 运行以上的代码块完成模型的训练，在test_dataset数据集上能够达到90%以上的准确率，如有需要可自行调整相关参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节主要参考：\n",
    "\n",
    "1. Mindspore 快速入门：[https://www.mindspore.cn/tutorials/zh-CN/r2.2/beginner/quick_start.html](https://www.mindspore.cn/tutorials/zh-CN/r2.2/beginner/quick_start.html)\n",
    "2. 动手学深度学习: 6.6节卷积神经网络LeNet [https://zh.d2l.ai/chapter_convolutional-neural-networks/lenet.html](https://zh.d2l.ai/chapter_convolutional-neural-networks/lenet.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('d2l')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "fc7e28f69d216f8b7620a4679cb249629759ba85060d9a3f52dcf6d424c3807d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
