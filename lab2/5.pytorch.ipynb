{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtOy2kScyxuH",
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "##  5、PyTorch框架\n",
    "\n",
    "本节我们使用学术界用的比较多的PyTorch框架基于CIFAR-10数据集进行模型的训练。PyTorch 安装请参考官方手册[PyTorch START LOCALLY](https://pytorch.org/get-started/locally/)，根据所用的环境选择对应的安装方式。\n",
    "\n",
    "PyTorch模型构建有低、中、高3个层次的API：\n",
    "\n",
    "1. 低阶API，主要包括张量操作，在低阶API层次上，可以把Pytorch当做一个增强版的numpy来使用。Pytorch提供的方法比numpy更全面。低阶API同前面我们自己实现的各种前向计算、反向计算函，区别是pytorch都已经实现好了，主要在torch.nn.functional模块中。低阶API构建模型的方法不做详细介绍。\n",
    "\n",
    "2. 中阶API，基于`nn.Module`构建模型并完成训练。\n",
    "\n",
    "3. 高阶API，基于`nn.Sequential`构建模型并完成训练。\n",
    "\n",
    "\n",
    "三个不同层次的API在灵活性和便利性上面有以下特点：\n",
    "\n",
    "| API           | Flexibility | Convenience |\n",
    "|---------------|-------------|-------------|\n",
    "| Barebone      | High        | Low         |\n",
    "| `nn.Module`     | High        | Medium      |\n",
    "| `nn.Sequential` | Low         | High        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本节用到的pytorch包，框架名是pytorch，包名是torch\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F  # useful stateless functions\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AA4BGYiSyxuO",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "如果要用GPU，确保所在的环境有GPU并且安装了合适的GPU的驱动和CUDA版本，并修改USE_GPU=True，再执行以下代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4212,
     "status": "ok",
     "timestamp": 1630057464952,
     "user": {
      "displayName": "MANTAS BIRŠKUS",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh4p74VzcLbLKZFYUnmVzxKmHhFZC9ouHcczEsEmQ=s64",
      "userId": "00995227095641424292"
     },
     "user_tz": -180
    },
    "id": "RAcHLiSxyxuP",
    "outputId": "baf7b2e3-47b6-49b0-d8ee-bc0743531de5",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "USE_GPU = False\n",
    "dtype = torch.float32 # 数据类型全部使用float32\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibP1gCTkyxuQ"
   },
   "source": [
    "### 5.1 CIFAR10数据集\n",
    "\n",
    "[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html )，总共有60000张标注好的32*32大小的彩色图片，总共10个类别，每个类别6000张。其中50000张训练集，10000张测试集，分成5个training batches和一个test batch文件，每个batch文件包含10000张图片。\n",
    "\n",
    "点击[python version](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)下载CIFAR-10的python版本，下载下来是tar.gz压缩包，解压到cifar-10-batches-py目录，会看到data_batch1~data_batch5 五个训练集文件和一个test_batch测试集文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用shell脚本下载CIFAR10数据文件，有160多MB，下载时间跟网络状况有关\n",
    "# 如有warning可执行：sudo apt install python3-pickleshare\n",
    "# windows下不支持shell脚本运行，可参照get_datasets.sh中的命令手动下载数据到指定目录并解压\n",
    "%cd src/dataset/\n",
    "!bash get_datasets.sh\n",
    "%cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw CIFAR-10 data.\n",
    "from src.util import load_CIFAR10,download_CIFAR10\n",
    "\n",
    "cifar10_root = 'src/dataset/'\n",
    "cifar10_dir = cifar10_root + 'cifar-10-batches-py'\n",
    "# download_CIFAR10(cifar10_root)\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "import matplotlib.pyplot as plt\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR10是一个比较经典的数据集，PyTorch提供非常便利的工具用于加载和各种预处理。CIFAR10是彩色图片，模型的训练难度比MNIST大。训练过程中将CIFAR-10的训练集拿了1000张出来做验证集，验证集用于验证和挑选合适的超参数，可以在模型训练过程中反复被用到，而测试集只应该被使用一次。\n",
    "\n",
    "以下代码块用pytorch自带的加载工具加载CIFAR10并划分成了训练集、验证集和测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11317,
     "status": "ok",
     "timestamp": 1630057478688,
     "user": {
      "displayName": "MANTAS BIRŠKUS",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh4p74VzcLbLKZFYUnmVzxKmHhFZC9ouHcczEsEmQ=s64",
      "userId": "00995227095641424292"
     },
     "user_tz": -180
    },
    "id": "aToNEJK0yxuQ",
    "outputId": "5d6130b0-75ff-4efe-94b6-1c742b60b492",
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_TRAIN = 49000\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to\n",
    "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "cifar10_train = dset.CIFAR10('./src/dataset', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./src/dataset', train=True, download=True,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./src/dataset', train=False, download=True, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhgPMbk3yxur"
   },
   "source": [
    "### 5.2 Module API\n",
    "\n",
    "这一小节介绍使用Module API分别实现一个2层全连接网络和3层卷积网络的定义和训练。\n",
    "\n",
    "使用Module API构建模型的步骤如下:\n",
    "\n",
    "1. 定义一个新的模型类继承自 `nn.Module`，新的类名要能直观地提现模型的结构如 `TwoLayerFC`。\n",
    "\n",
    "2. 在构造函数 `__init__()`中定义所有的层，模型层直接用PyTorch提供的 `nn.Linear`、 `nn.Conv2d`等常用层, 只需传入层的超参数，层的权重会自动生成。记得要先调用 `super().__init__()` 函数。\n",
    "\n",
    "3. 实现模型的 `forward()` 方法, 描述模型的前向计算是怎么执行的，也决定了模型中各层的连接关系。\n",
    "\n",
    "PyTorch内置的层可以查阅手册[builtin layers doc](http://pytorch.org/docs/master/nn.html) 。\n",
    "定义好模型类后，就可以实例化模型。以下是使用Module API构建2层全连接网络的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60,
     "status": "ok",
     "timestamp": 1630057532380,
     "user": {
      "displayName": "MANTAS BIRŠKUS",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh4p74VzcLbLKZFYUnmVzxKmHhFZC9ouHcczEsEmQ=s64",
      "userId": "00995227095641424292"
     },
     "user_tz": -180
    },
    "id": "ODEUN7HXyxur",
    "outputId": "f59a63ee-be1c-45cb-ce50-69a932dfe577"
   },
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "\n",
    "class TwoLayerFC(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        # assign layer objects to class attributes\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # nn.init package contains convenient initialization methods\n",
    "        # http://pytorch.org/docs/master/nn.html#torch-nn-init \n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # forward always defines connectivity\n",
    "        x = flatten(x)\n",
    "        scores = self.fc2(F.relu(self.fc1(x)))\n",
    "        return scores\n",
    "\n",
    "def test_TwoLayerFC():\n",
    "    input_size = 50\n",
    "    x = torch.zeros((64, input_size), dtype=dtype)  # minibatch size 64, feature dimension 50\n",
    "    model = TwoLayerFC(input_size, 42, 10)\n",
    "    scores = model(x)\n",
    "    print(scores.size())  # you should see [64, 10]\n",
    "test_TwoLayerFC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4mkNJT7yxus"
   },
   "source": [
    "使用Module API构建卷积网络的过程同上，以3层的卷积模型为例，模型结构如下：\n",
    "\n",
    "1. 卷积层1： `channel_1`个输出通道， 5x5 大小， 零填充，填充大小是2\n",
    "2. ReLU激活\n",
    "3. 卷积层2： `channel_2`个输出通道，3x3 大小，零填充，填充大小是1\n",
    "4. ReLU激活\n",
    "5. 全连接层：`num_classes`个输出\n",
    "\n",
    "不需要显式定义softmax层，交叉熵损失函数cross_entropy会自动做softmax处理。实现3层卷积网络之后, 执行`test_ThreeLayerConvNet` 会输出scores的大小 `(64, 10)` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1630057532383,
     "user": {
      "displayName": "MANTAS BIRŠKUS",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh4p74VzcLbLKZFYUnmVzxKmHhFZC9ouHcczEsEmQ=s64",
      "userId": "00995227095641424292"
     },
     "user_tz": -180
    },
    "id": "module_output_shape",
    "outputId": "96db09f6-6e68-47d5-dd6a-47346558e176"
   },
   "outputs": [],
   "source": [
    "class ThreeLayerConvNet(nn.Module):\n",
    "    def __init__(self, in_channel, channel_1, channel_2, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # http://pytorch.org/docs/stable/nn.html#conv2d\n",
    "        self.conv1 = nn.Conv2d(in_channel, channel_1, 5, padding=2)\n",
    "        nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        self.conv2 = nn.Conv2d(channel_1, channel_2, 3, padding=1)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight)\n",
    "        self.fc = nn.Linear(channel_2 * 32 * 32, num_classes)\n",
    "        nn.init.kaiming_normal_(self.fc.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        scores = None\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        scores = self.fc(flatten(x))\n",
    "        \n",
    "        return scores\n",
    "\n",
    "\n",
    "def test_ThreeLayerConvNet():\n",
    "    x = torch.zeros((64, 3, 32, 32), dtype=dtype)  # minibatch size 64, image size [3, 32, 32]\n",
    "    model = ThreeLayerConvNet(in_channel=3, channel_1=12, channel_2=8, num_classes=10)\n",
    "    scores = model(x)\n",
    "    print(scores.size())  # you should see [64, 10]\n",
    "test_ThreeLayerConvNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xp7uccBtyxuv"
   },
   "source": [
    "下面块的`check_accuracy()`方法计算模型在指定数据集上的准确率，执行的是前向计算，无需累积梯度，所以在torch.no_grad()环境下执行，不做反向计算，不保留梯度。\n",
    "\n",
    "[torch.no_grad()](https://pytorch.org/docs/stable/generated/torch.no_grad.html)是一个禁止梯度计算的上下文管理器，调用torch.no_grad()后，变量的requires_grad属性会设置为False，会减少内存消耗，减少反向传播的计算。\n",
    "\n",
    "[model.eval()](https://pytorch.org/docs/stable/notes/autograd.html#evaluation-mode-nn-module-eval)只对特定的层有作用如Dropout, BatchNorm，eval模式是不启用这些层，如果没有这些层，model.eval()和model.train()没有任何区别。model.eval()跟梯度计算是没有关系的，不要与torch.no_grad()的作用弄混。\n",
    "\n",
    "详细可参考：\n",
    "\n",
    "1. [Evaluating pytorch models: `with torch.no_grad` vs `model.eval()`](https://stackoverflow.com/questions/55627780/evaluating-pytorch-models-with-torch-no-grad-vs-model-eval)\n",
    "\n",
    "2. [PyTorch AUTOGRAD MECHANICS](https://pytorch.org/docs/stable/notes/autograd.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBGGYFqiyxuv"
   },
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qIiVPDxyxuv"
   },
   "source": [
    "`train_model()`方法进行模型的训练：计算损失、反向传播求梯度、使用优化算法（优化器）更新模型权重。参数optimizer指定权重优化方法，除了随机梯度下降（SGD）外还有Momentum、Adam等方法，pytorch优化器的实现在`torch.optim`包中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQxgYxfDyxuw"
   },
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    print_every = 200\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        print(\"Epochs :\", e)\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                check_accuracy(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pr__ynn0yxuw"
   },
   "source": [
    "这里以2层的全连接网络TwoLayerFC训练为例：\n",
    "1. 定义模型TwoLayerFC，只需要传入输入数据大小、hidden_layer_size、分类类别数\n",
    "2. 使用SGD优化器\n",
    "3. 调用train_model进行模型训练，训练一个epoch之后预计可以达到40%的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17381,
     "status": "ok",
     "timestamp": 1630057557667,
     "user": {
      "displayName": "MANTAS BIRŠKUS",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh4p74VzcLbLKZFYUnmVzxKmHhFZC9ouHcczEsEmQ=s64",
      "userId": "00995227095641424292"
     },
     "user_tz": -180
    },
    "id": "ju1ymd-Lyxuw",
    "outputId": "02d13cce-8674-469a-8c5c-20ba2a29e8f8"
   },
   "outputs": [],
   "source": [
    "hidden_layer_size = 4000\n",
    "learning_rate = 1e-2\n",
    "model = TwoLayerFC(3 * 32 * 32, hidden_layer_size, 10)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_model(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xKLq5kUyxu0"
   },
   "source": [
    "卷积神经网络的训练跟TwoLayerFC训练过程一样，训练一个epoch之后预计可以达到45%的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18493,
     "status": "ok",
     "timestamp": 1630057577931,
     "user": {
      "displayName": "MANTAS BIRŠKUS",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh4p74VzcLbLKZFYUnmVzxKmHhFZC9ouHcczEsEmQ=s64",
      "userId": "00995227095641424292"
     },
     "user_tz": -180
    },
    "id": "module_accuracy",
    "outputId": "6188fb49-1d4f-4a2c-bc25-5af27c4757b7"
   },
   "outputs": [],
   "source": [
    "learning_rate = 3e-3\n",
    "channel_1 = 32\n",
    "channel_2 = 16\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "model = ThreeLayerConvNet(3, channel_1, channel_2, 10)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_model(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面代码有`loss.backward()`等操作涉及到自动求导，Pytorch的自动求导由autograd包实现。下面代码展示了PyTorch简单的求导的应用，计算结果跟前面我们自行实现sigmoid结果一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对标量求导\n",
    "x1=torch.tensor(1.0, requires_grad=True)\n",
    "torch_sigmoid = torch.nn.Sigmoid()\n",
    "y = torch_sigmoid(x1)\n",
    "y.backward()\n",
    "print('x1 grad:', x1.grad)\n",
    "\n",
    "# 对向量求导\n",
    "x2 = torch.tensor([-1.0, 1.0, 2.0], requires_grad=True)\n",
    "torch_sigmoid = torch.nn.Sigmoid()\n",
    "y = torch_sigmoid(x2)\n",
    "y.backward(torch.ones(x2.shape))  #或者通过y.sum().backward()求导。\n",
    "print('x2 grad:', x2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoVWOAguyxu2"
   },
   "source": [
    "## 5.3 Sequential API\n",
    "\n",
    "对于简单的模型比如前馈神经网络，使用`nn.Module` API定义模型仍然需要3步：定义新的模型类、在构造函数`__init__`中定义层、在`forward()`方法中调用每个层。\n",
    "\n",
    "PyTorch提供了一个容器类`nn.Sequential`可以将以上3步合成一步，更加方便，但没有`nn.Module`灵活，如果要定义一个复杂的网络结构则不可行，但`nn.Sequential`覆盖了大部分的场景，尤其是简单堆叠的前馈神经网络。\n",
    "\n",
    "下面使用`nn.Sequential`重新定义Two-Layer Network并完成训练，利用变长参数按层顺序构建即可，模型层按照定义的顺序添加，非常直观。 另外接受OrderedDict参数进行模型定义。如果模型已经定义好了，还可通过add_module继续添加层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17746,
     "status": "ok",
     "timestamp": 1630057595672,
     "user": {
      "displayName": "MANTAS BIRŠKUS",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh4p74VzcLbLKZFYUnmVzxKmHhFZC9ouHcczEsEmQ=s64",
      "userId": "00995227095641424292"
     },
     "user_tz": -180
    },
    "id": "9hXd-cj3yxu2",
    "outputId": "867de249-a184-414d-e468-bc16ac926587"
   },
   "outputs": [],
   "source": [
    "# We need to wrap `flatten` function in a module in order to stack it\n",
    "# in nn.Sequential\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)\n",
    "\n",
    "hidden_layer_size = 4000\n",
    "learning_rate = 1e-2\n",
    "\n",
    "model = nn.Sequential(\n",
    "    Flatten(),\n",
    "    nn.Linear(3 * 32 * 32, hidden_layer_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_layer_size, 10),\n",
    ")\n",
    "\n",
    "# you can use Nesterov momentum in optim.SGD\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                     momentum=0.9, nesterov=True)\n",
    "\n",
    "train_model(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcEWC00xyxu3"
   },
   "source": [
    "下面使用`nn.Sequential`重新定义3层卷积神经网络并完成训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18515,
     "status": "ok",
     "timestamp": 1630057614180,
     "user": {
      "displayName": "MANTAS BIRŠKUS",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh4p74VzcLbLKZFYUnmVzxKmHhFZC9ouHcczEsEmQ=s64",
      "userId": "00995227095641424292"
     },
     "user_tz": -180
    },
    "id": "sequential_accuracy",
    "outputId": "07f0a1a7-358e-44b3-ea6a-0d673811180b"
   },
   "outputs": [],
   "source": [
    "channel_1 = 32\n",
    "channel_2 = 16\n",
    "learning_rate = 1e-2\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, channel_1, 5, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(channel_1, channel_2, 3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    Flatten(),\n",
    "    nn.Linear(channel_2 * 32 * 32, 10)\n",
    ")\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "train_model(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 自主设计网络\n",
    "\n",
    "下面请自主实现一个神经网络，在CIFAR-10数据集上完成训练，可以使用任意的模型结构、超参数、损失函数和优化器等，要求在训练10个epoch内在验证集上至少达到70%的准确率。可以使用经典的模型，但不能直接使用modelzoo中定义好的或者训练好的模型。 \n",
    "\n",
    "可能需要参考的资料：\n",
    "\n",
    "* Layers in torch.nn package: http://pytorch.org/docs/stable/nn.html\n",
    "* Activations: http://pytorch.org/docs/stable/nn.html#non-linear-activations\n",
    "* Loss functions: http://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "* Optimizers: http://pytorch.org/docs/stable/optim.html\n",
    "* API doc: http://pytorch.org/docs/stable/index.html\n",
    "* PyTorch discuss: https://discuss.pytorch.org/\n",
    "* tutorial: https://github.com/jcjohnson/pytorch-examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osHaEcx1yxu9"
   },
   "source": [
    "训练集上完成训练后，最后在测试集上进行测试，注意测试集只能用一次，用于模型最后的泛化能力评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8425,
     "status": "ok",
     "timestamp": 1630060931914,
     "user": {
      "displayName": "MANTAS BIRŠKUS",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh4p74VzcLbLKZFYUnmVzxKmHhFZC9ouHcczEsEmQ=s64",
      "userId": "00995227095641424292"
     },
     "user_tz": -180
    },
    "id": "YsZWjU6jyxu_",
    "outputId": "b7e5e33d-3ddf-486c-86d0-99aed09be494"
   },
   "outputs": [],
   "source": [
    "best_model = model\n",
    "check_accuracy(loader_test, best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节要完成的内容：\n",
    "1. 安装PyTorch，运行以上代码，熟悉PyTorch的基础使用\n",
    "2. 自定义一个模型实现CIFAR10分类，使用PyTorch完成训练，要求10个epoch内能在验证集上至少达到70%的准确率。\n",
    "3. 在实验报告中，分别给出本节用到的两层全连接网络、三层卷积网络、自定义网络的参数量，使用[pytorch-summary](https://github.com/sksq96/pytorch-summary)等工具或者自行逐层输出进行结果验证。\n",
    "\n",
    "主要参考：\n",
    "1. cs231n: http://cs231n.stanford.edu/\n",
    "2. 《深度学习入门：基于Python的理论与实现》斋藤康毅著"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PyTorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.18 ('lab2024')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "e3cccdbd36e10d67412b2cf50b3d95da4006c0be4f4996637d1708e499edddb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
